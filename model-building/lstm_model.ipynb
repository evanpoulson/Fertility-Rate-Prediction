{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) for Fertility Rate Prediction\n",
    "\n",
    "This notebook implements an LSTM network to predict fertility rates using sequential time-series data.\n",
    "\n",
    "## Why LSTM?\n",
    "LSTMs are an advanced type of RNN that solve the \"vanishing gradient\" problem. They have special gates that allow them to:\n",
    "- Remember important information from long ago\n",
    "- Forget irrelevant information\n",
    "- Learn long-term dependencies better than simple RNNs\n",
    "\n",
    "## Architecture:\n",
    "- Input: Sequences of 5 years × 29 features\n",
    "- LSTM layers: LSTM cells with dropout\n",
    "- Output layer: Single neuron (regression)\n",
    "- LSTMs typically outperform simple RNNs on time-series tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed sequence data\n",
    "data_dir = '../data/processed'\n",
    "\n",
    "X_train_seq = np.load(f'{data_dir}/X_train_seq.npy')\n",
    "y_train_seq = np.load(f'{data_dir}/y_train_seq.npy')\n",
    "X_val_seq = np.load(f'{data_dir}/X_val_seq.npy')\n",
    "y_val_seq = np.load(f'{data_dir}/y_val_seq.npy')\n",
    "X_test_seq = np.load(f'{data_dir}/X_test_seq.npy')\n",
    "y_test_seq = np.load(f'{data_dir}/y_test_seq.npy')\n",
    "\n",
    "# Load configuration\n",
    "with open(f'{data_dir}/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Sequence data loaded successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set:   X={X_train_seq.shape}, y={y_train_seq.shape}\")\n",
    "print(f\"Validation set: X={X_val_seq.shape}, y={y_val_seq.shape}\")\n",
    "print(f\"Test set:       X={X_test_seq.shape}, y={y_test_seq.shape}\")\n",
    "print(f\"\\nSequence structure:\")\n",
    "print(f\"  - Sequence length: {X_train_seq.shape[1]} time steps\")\n",
    "print(f\"  - Features per time step: {X_train_seq.shape[2]}\")\n",
    "print(f\"\\nData split years:\")\n",
    "print(f\"  Train: {config['train_years']}\")\n",
    "print(f\"  Val:   {config['val_years']}\")\n",
    "print(f\"  Test:  {config['test_years']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequence metadata to understand the data better\n",
    "train_meta = pd.read_csv(f'{data_dir}/train_seq_metadata.csv')\n",
    "val_meta = pd.read_csv(f'{data_dir}/val_seq_metadata.csv')\n",
    "test_meta = pd.read_csv(f'{data_dir}/test_seq_metadata.csv')\n",
    "\n",
    "print(\"Example sequences from training data:\")\n",
    "print(\"=\"*80)\n",
    "print(train_meta.head(10))\n",
    "print(f\"\\nEach sequence uses {config['sequence_length']} years of history to predict the next year's fertility rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the LSTM Model\n",
    "\n",
    "We'll create an LSTM with:\n",
    "- LSTM layers (more sophisticated than SimpleRNN)\n",
    "- Dropout and recurrent dropout for regularization\n",
    "- Dense layers for final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(sequence_length, n_features, lstm_units=[64, 32], dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Build an LSTM network for time-series regression.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequence_length: Number of time steps in each sequence\n",
    "    - n_features: Number of features per time step\n",
    "    - lstm_units: List of units in each LSTM layer\n",
    "    - dropout_rate: Dropout rate for regularization\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(sequence_length, n_features)))\n",
    "    \n",
    "    # LSTM layers\n",
    "    for i, units in enumerate(lstm_units[:-1]):\n",
    "        # return_sequences=True means output sequence for next LSTM layer\n",
    "        # recurrent_dropout helps prevent overfitting in recurrent connections\n",
    "        model.add(layers.LSTM(\n",
    "            units,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate * 0.5,  # Typically lower than regular dropout\n",
    "            name=f'lstm_{i+1}'\n",
    "        ))\n",
    "    \n",
    "    # Last LSTM layer (return_sequences=False to output single vector)\n",
    "    model.add(layers.LSTM(\n",
    "        lstm_units[-1],\n",
    "        return_sequences=False,\n",
    "        dropout=dropout_rate,\n",
    "        recurrent_dropout=dropout_rate * 0.5,\n",
    "        name=f'lstm_{len(lstm_units)}'\n",
    "    ))\n",
    "    \n",
    "    # Dense layers for final prediction\n",
    "    model.add(layers.Dense(16, activation='relu', name='dense_1'))\n",
    "    model.add(layers.Dropout(dropout_rate, name='dropout_final'))\n",
    "    \n",
    "    # Output layer (single neuron for regression)\n",
    "    model.add(layers.Dense(1, name='output'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"LSTM model builder function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "lstm_model = build_lstm_model(\n",
    "    sequence_length=X_train_seq.shape[1],\n",
    "    n_features=X_train_seq.shape[2],\n",
    "    lstm_units=[64, 32],\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"LSTM Model Architecture:\")\n",
    "print(\"=\"*80)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "keras.utils.plot_model(\n",
    "    lstm_model, \n",
    "    show_shapes=True, \n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving models\n",
    "import os\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=25,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f'{model_dir}/lstm_best.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callback_list = [early_stopping, model_checkpoint, reduce_lr]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  - EarlyStopping (patience=25)\")\n",
    "print(\"  - ModelCheckpoint (save best model)\")\n",
    "print(\"  - ReduceLROnPlateau (reduce LR on plateau)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting LSTM training...\")\n",
    "print(\"=\"*80)\n",
    "print(\"Note: LSTMs have more parameters than simple RNNs, so training may take longer.\\n\")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('LSTM Model Loss Over Epochs', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Mean Absolute Error')\n",
    "axes[1].set_title('LSTM Model MAE Over Epochs', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best epoch\n",
    "best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "best_val_loss = np.min(history.history['val_loss'])\n",
    "print(f\"\\nBest model at epoch {best_epoch} with validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_lstm_model = keras.models.load_model(f'{model_dir}/lstm_best.keras')\n",
    "print(\"Best LSTM model loaded!\")\n",
    "\n",
    "# Make predictions on all sets\n",
    "y_train_pred = best_lstm_model.predict(X_train_seq, verbose=0).flatten()\n",
    "y_val_pred = best_lstm_model.predict(X_val_seq, verbose=0).flatten()\n",
    "y_test_pred = best_lstm_model.predict(X_test_seq, verbose=0).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, set_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{set_name} Set Metrics:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LSTM MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_metrics = calculate_metrics(y_train_seq, y_train_pred, \"Training\")\n",
    "val_metrics = calculate_metrics(y_val_seq, y_val_pred, \"Validation\")\n",
    "test_metrics = calculate_metrics(y_test_seq, y_test_pred, \"Test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "datasets = [\n",
    "    (y_train_seq, y_train_pred, 'Training', 'blue'),\n",
    "    (y_val_seq, y_val_pred, 'Validation', 'orange'),\n",
    "    (y_test_seq, y_test_pred, 'Test', 'green')\n",
    "]\n",
    "\n",
    "for idx, (y_true, y_pred, name, color) in enumerate(datasets):\n",
    "    # Scatter plot\n",
    "    axes[idx].scatter(y_true, y_pred, alpha=0.5, s=20, color=color)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[idx].set_xlabel('Actual Fertility Rate')\n",
    "    axes[idx].set_ylabel('Predicted Fertility Rate')\n",
    "    axes[idx].set_title(f'{name} Set Predictions (LSTM)', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Add R² to plot\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    axes[idx].text(0.05, 0.95, f'R² = {r2:.4f}', \n",
    "                   transform=axes[idx].transAxes,\n",
    "                   verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (y_true, y_pred, name, color) in enumerate(datasets):\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    axes[idx].scatter(y_pred, residuals, alpha=0.5, s=20, color=color)\n",
    "    axes[idx].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_xlabel('Predicted Fertility Rate')\n",
    "    axes[idx].set_ylabel('Residuals')\n",
    "    axes[idx].set_title(f'{name} Set Residuals (LSTM)', fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of errors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (y_true, y_pred, name, color) in enumerate(datasets):\n",
    "    errors = y_true - y_pred\n",
    "    \n",
    "    axes[idx].hist(errors, bins=30, color=color, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[idx].set_xlabel('Prediction Error')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'{name} Set Error Distribution (LSTM)', fontweight='bold')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add mean and std to plot\n",
    "    axes[idx].text(0.05, 0.95, f'Mean = {errors.mean():.4f}\\nStd = {errors.std():.4f}', \n",
    "                   transform=axes[idx].transAxes,\n",
    "                   verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Temporal Predictions\n",
    "\n",
    "Visualize how well the LSTM predicts over time for sample countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions for sample countries\n",
    "sample_countries = test_meta['Country'].unique()[:6]  # First 6 countries in test set\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, country in enumerate(sample_countries):\n",
    "    # Get data for this country\n",
    "    country_mask = test_meta['Country'] == country\n",
    "    country_years = test_meta[country_mask]['Target_Year'].values\n",
    "    country_true = y_test_seq[country_mask]\n",
    "    country_pred = y_test_pred[country_mask]\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(country_years, country_true, marker='o', label='Actual', linewidth=2)\n",
    "    axes[idx].plot(country_years, country_pred, marker='s', label='Predicted', linewidth=2, alpha=0.7)\n",
    "    axes[idx].set_xlabel('Year')\n",
    "    axes[idx].set_ylabel('Fertility Rate')\n",
    "    axes[idx].set_title(f'{country}', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('LSTM Predictions Over Time (Test Set - Sample Countries)', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "results = {\n",
    "    'train': {'y_true': y_train_seq, 'y_pred': y_train_pred, 'metrics': train_metrics},\n",
    "    'val': {'y_true': y_val_seq, 'y_pred': y_val_pred, 'metrics': val_metrics},\n",
    "    'test': {'y_true': y_test_seq, 'y_pred': y_test_pred, 'metrics': test_metrics}\n",
    "}\n",
    "\n",
    "# Save as pickle\n",
    "with open(f'{model_dir}/lstm_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# Save metrics as JSON\n",
    "metrics_summary = {\n",
    "    'train': train_metrics,\n",
    "    'val': val_metrics,\n",
    "    'test': test_metrics,\n",
    "    'model_config': {\n",
    "        'lstm_units': [64, 32],\n",
    "        'dropout_rate': 0.3,\n",
    "        'recurrent_dropout_rate': 0.15,\n",
    "        'sequence_length': config['sequence_length'],\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'total_epochs': len(history.history['loss']),\n",
    "        'best_epoch': int(best_epoch)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{model_dir}/lstm_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print(\"Results saved successfully!\")\n",
    "print(f\"  - Model: {model_dir}/lstm_best.keras\")\n",
    "print(f\"  - Results: {model_dir}/lstm_results.pkl\")\n",
    "print(f\"  - Metrics: {model_dir}/lstm_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LSTM MODEL - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"  - Input: {X_train_seq.shape[1]} time steps × {X_train_seq.shape[2]} features\")\n",
    "print(f\"  - LSTM layers: [64, 32] units\")\n",
    "print(f\"  - Total parameters: {lstm_model.count_params():,}\")\n",
    "print(f\"  - Regularization: Dropout (0.3) + Recurrent Dropout (0.15)\")\n",
    "\n",
    "print(\"\\nTraining:\")\n",
    "print(f\"  - Total epochs: {len(history.history['loss'])}\")\n",
    "print(f\"  - Best epoch: {best_epoch}\")\n",
    "print(f\"  - Batch size: 32\")\n",
    "print(f\"  - Optimizer: Adam (lr=0.001)\")\n",
    "\n",
    "print(\"\\nFinal Performance:\")\n",
    "print(\"\\n  Training Set:\")\n",
    "print(f\"    RMSE: {train_metrics['rmse']:.4f}\")\n",
    "print(f\"    MAE:  {train_metrics['mae']:.4f}\")\n",
    "print(f\"    R²:   {train_metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"\\n  Validation Set:\")\n",
    "print(f\"    RMSE: {val_metrics['rmse']:.4f}\")\n",
    "print(f\"    MAE:  {val_metrics['mae']:.4f}\")\n",
    "print(f\"    R²:   {val_metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"\\n  Test Set:\")\n",
    "print(f\"    RMSE: {test_metrics['rmse']:.4f}\")\n",
    "print(f\"    MAE:  {test_metrics['mae']:.4f}\")\n",
    "print(f\"    R²:   {test_metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ LSTM training complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
